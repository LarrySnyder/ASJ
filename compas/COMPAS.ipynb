{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1g5k25U3yEZJ_URPh_VaTGEW0TM4pcgAm",
      "authorship_tag": "ABX9TyPMOjy4wj+2nt6nURntfKqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/ASJ/blob/main/compas/COMPAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPAS\n",
        "\n",
        "This file is read-only. To work with it, you first need to **save a copy to your Google Drive:**\n",
        "\n",
        "1. Go to the *File* menu. (The *File* menu inside the notebook, right below the filenameâ€”not the *File* menu in your browser, at the top of your screen.)\n",
        "2. Choose *Save a copy in Drive*. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "3. Colab should open up a new browser tab with your copy of the notebook. Double-click the filename at the top of the window and rename it `COMPAS [your name(s)]`. \n",
        "4. Close the original read-only notebook in your browser.\n"
      ],
      "metadata": {
        "id": "pdAvxkGROQfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ‘“ **Note:** This notebook is part of the *Algorithms and Social Justice* course at Lehigh University, Profs. Larry Snyder and Suzanne Edwards.\n",
        "---\n"
      ],
      "metadata": {
        "id": "n92vqZCZOWyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ“š **Reference:** Portions of this notebook are adapted from the *Data-4ac* course at the University of Californiaâ€“Berkeley, Spring 2021, Prof. Margarita Boenig-Liptsin (available at https://github.com/ds-modules/data-4ac) and from Aaron Fraenkel, *Fairness and Algorithmic Decision* making, UCSD course DSC 167 (available at https://afraenkel.github.io/fairness-book/content/04-compas.html).\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zd11MweA6cYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "Yxucb1PS55Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision making within the United States criminal justice system relies heavily on risk assessment, which determines the potential risk that a released defendant will fail to appear in court, or will cause harm to the public. Judges use these assessments to decide whether bail can be set, or whether a defendant should be detailed until their trial. \n",
        "\n",
        "While risk assessment is not a new concept in the legal system, the use of risk scores determined by an algorithm is gaining prevalence and support. Proponents promote the use of risk scores to guide judges in their decision making, arguing that machine learning could improve efficiency and accountability and reduce bias in decision making compared with human judgement ([Henry 2019](https://theappeal.org/risk-assessment-explained/)). \n",
        "\n",
        "On the other hand, critical voices raise the concern that biases can creep into these algorithms at any point in the process, and that algorithms are often applied to the wrong situations. Further, they exacerbate the racism embedded deep within the criminal justice system by perpetuating inequalities found in historical data ([Henry 2019](https://theappeal.org/risk-assessment-explained/)).\n",
        "\n",
        "In the debate about the use of risk assessment algorithms, people have used data analysis to determine the extent to which these algorithms are fair to different groups of people. The [ProPublica article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) that we read in class uses data analysis to argue that the COMPAS risk assessment algorithm is racially biased. Northpointe, the company that created COMPAS, offered a different analysis, still based on data science, to argue that the results are in fact not biased. ProPublica and Northpointe used different **metrics** (ways of quantifying concepts like \"fairness\") to support their arguments. \n",
        "\n",
        "In this notebook, **you will explore the actual data used by ProPublica, examining their arguments and analyses** to gain a deeper understanding of the technical and societal interpretations and implications of fairness. (In the next notebook, you'll explore Northpointe's \"rebuttal\" analysis.)\n",
        "\n",
        "This is a longer notebook than we've used so far. You can view an outline of the notebook by clicking the \"bullet list\" icon at the top of the left-hand toolbar."
      ],
      "metadata": {
        "id": "aPx9tyDk68F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ‘“ **Note:** When we discuss \"bias\" in this notebook, we define it generally as prejudice or an inclination in favor of one person, thing, or group compared to another. In the context of machine learning, \"bias\" has a more narrow [mathematical meaning](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but that is not the definition we will use here.\n",
        "---"
      ],
      "metadata": {
        "id": "E94iuGG99N6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Python Stuff\n",
        "\n",
        "First we need to import that Python packages that we'll need for our analysis. We've used all of these packages before.\n",
        "\n",
        "* `pandas` (pronounced like the animal) handles data\n",
        "* `numpy` (\"num-pie\") does numerical computations\n",
        "* `seaborn` does data visualization\n",
        "* `sklearn` (\"s-k-learn\") does machine learning\n",
        "* `matplotlib` (\"mat-plot-libe\") does graphing\n"
      ],
      "metadata": {
        "id": "4bKbS818_AVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RmwhufvDAFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## COMPAS: Why It Was Created and How It Exists in the Court System <a id=\"compas\"></a>\n",
        "\n",
        "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a commercial tool produced by the for-profit company Northpointe (now called Equivant). It is known as a **recidivism risk assessment system.** Tools like COMPAS are used to **predict the risk of future crimes for an individual who has entered the U.S. criminal justice system, by outputting a risk score from 1 to 10.**\n",
        "\n",
        "While COMPAS was initially intended to aid decisions made by probation officers on treatment and supervision of those who are incarcerated, Northpointe has since emphasized the scalability of the tool to \"fit the needs of many different decision points,\" including pre-screening assessments, pre-trial release decisions (whether or not to hold an arrested individual in jain until their trial), and post-trial next steps for the defendant. These algorithms are believed by many to hold the power to relieve the court system of unfair human bias from criminal justice officials.\n",
        "\n"
      ],
      "metadata": {
        "id": "XMcCKsQ8A2_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 0a**\n",
        "\n",
        "List 3 parties who are affected by the COMPAS tool. In what ways are they affected? (Can you think of impacts beyond those in the courtroom for at least one of your examples?)"
      ],
      "metadata": {
        "id": "unyK7AotJcOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "bXabi3KuJs7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 0b**\n",
        "\n",
        "Based on your initial reading, what is one problem of the criminal justice system that the COMPAS tool could potentially alleviate? What is one potential problem that using the COMPAS algorithm could introduce?"
      ],
      "metadata": {
        "id": "O9YcCLp6JuUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "gwb5NuXZJ3jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Coproduction of Justice and Data<a id=\"coproduction\"></a>"
      ],
      "metadata": {
        "id": "QMMtjFOFJ4j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Methods of Data Collection\n",
        "\n",
        "Before a risk score is determined for a defendant, the defendant is asked to fill out a questionnaire with questions meant to help predict the defendant's level of risk. Let's take a look at this questionnaire to get a better understanding of what goes into determining a risk score. [Here](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html) is a link to a sample questionnaire.\n"
      ],
      "metadata": {
        "id": "IXOilZkdKKmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 1a**\n",
        "\n",
        "What aspects of the questionnaire were particularly striking to you?"
      ],
      "metadata": {
        "id": "iczPd-ZnK-QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "9_wuLsKiLD1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1b**\n",
        "\n",
        "Does the questionnaire ask explicitly about race? If not, is race still embedded in the questionnaire? Explain."
      ],
      "metadata": {
        "id": "3xyEEzKWLGj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "pPkb6qZdLcwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Understanding the Data"
      ],
      "metadata": {
        "id": "gynS2hvjLrm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the data that were obtained and utilized by ProPublica in their own analysis of the COMPAS tool. They used Broward County, Florida public records of people who were scored in the COMPAS system between 2013 and 2014 ([ProPublica 2016](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)). ProPublica has made the dataset available on GitHub (a website for sharing code, data, and other files), and we'll be loading the data directly from there.\n",
        "\n",
        "First, we'll load the dataset from ProPublica's GitHub repository. In the same command to load the data, we'll also specify which columns we want to include. We'll select only the data that ProPublica used in their study, such as severity of the charge, number of priors, demographics, COMPAS scores, and whether each person was accused of another crime within two years.\n",
        "\n",
        "---\n",
        "> ðŸ‘“ **Note:** The dataset contains the full names of the people included. We're omitting these columns out of respect for these people's privacy. However, it's worth reflecting on what it means for their full names to be included in a publicly available dataset, even one that was posted by ProPublica in the interest of studying fairness.\n",
        "---\n"
      ],
      "metadata": {
        "id": "SymwtJeQWTk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from ProPublica's GitHub repository.\n",
        "# Specify the columns to include.\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv',\n",
        "                   usecols=[\"age\", \"c_charge_degree\", \"race\", \"age_cat\", \"score_text\", \"sex\", \"priors_count\", \n",
        "                    \"days_b_screening_arrest\", \"decile_score\", \"is_recid\", \"two_year_recid\", \"c_jail_in\", \"c_jail_out\"])"
      ],
      "metadata": {
        "id": "aDyWNRVvOkpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ProPublica removed any cases in which the criminal charge was not within 30 days of the COMPAS score, since those cases were harder to match the COMPAS score with the correct criminal case. We'll do the same, to follow ProPublica's analysis. We're left with 6172 rows in the dataset."
      ],
      "metadata": {
        "id": "JvjWDLXSOuyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data = data.query('days_b_screening_arrest <= 30 & days_b_screening_arrest >= -30')\n",
        "len(pp_data)"
      ],
      "metadata": {
        "id": "at1csdATRVRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also include only rows for defendants whose \"race\" column is either \"Caucasian\" or \"African-American\", since those are the only \"race\" values considered in the ProPublica study. This leaves us with 5278 rows. (The `|` in the condition means \"or\".)"
      ],
      "metadata": {
        "id": "nPpDiIgenxNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data = pp_data.query('race == \"Caucasian\" | race == \"African-American\"')\n",
        "len(pp_data)"
      ],
      "metadata": {
        "id": "n4yUgrfkn8eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a look at (a handful of rows of) the ProPublica dataset."
      ],
      "metadata": {
        "id": "2Uu5cOKZRlhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data"
      ],
      "metadata": {
        "id": "HlOIKe0cRc-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> âš ï¸ **Important:** The dataset used by the COMPAS model and the dataset used by ProPublica analysis are completely different. \n",
        ">\n",
        ">* **COMPAS** uses confidential, proprietary data for each person based on the answers to the questionnaire. It uses an algorithm that takes this dataset as an input and gives a risk assessment (COMPAS score) as an output.\n",
        "* **ProPublica** used data about the same defendants but containing public information about arrest records, COMPAS scores, and so on. ProPublica used data science algorithms to analyze fairness in the risk assessments.\n",
        ">\n",
        "> There is information in the **COMPAS** data that is not in the **ProPublica** data, such as how many times the person has violated their parole or how many of their friends are gang members. And there is information in the **ProPublica** data that is not in the **COMPAS** data, such as the COMPAS score and whether the person was accused of another crime within two years.\n",
        ">\n",
        "> The dataset we are working with in this notebook is the **ProPublica** dataset.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EyA8VdQESas_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ProPublica's Perspective<a id=\"propublica\"></a>\n",
        "\n",
        "### What is ProPublica?\n",
        "\n",
        "[ProPublica](https://www.propublica.org/about/) is a nonprofit organization that \"produces investigative journalism with moral force.\" ProPublica was founded as a nonpartisan newsroom aiming to expose and question abuses of power, justice, and public trust, often by systems and institutions deeply ingrained in the U.S.\n",
        "\n",
        "In 2016, ProPublica investigated the COMPAS algorithm to assess the accuracy of and potential racial bias within the tool, as it became more popular with the United States court system nationwide. In their analysis, ProPublica  used data from defendants with risk scores from Broward County from 2013 to 2014 to test for statistical differences in outcomes for Black and white defendants, which ultimately highlighted racial disparities that exist within the algorithm. ProPublica came to the conclusion that COMPAS utilizes data from a criminal justice system with a history of racial injustices, thus continuing to disproportionately target and arrest Black people in comparison to their white counterparts. While the COMPAS algorithm treats racial groups similarly in a certain sense (which we will explore below), which may make it appear to be neutral, ProPublica's data analysis and reporting emphasized the bias against Black defendants and their communities that COMPAS produced from this line of thinking, a claim that Northpointe has disputed (as we will see later).\n",
        "\n",
        "Let's retrace ProPublica's statistical analysis in order to better understand ProPublica's argument and engage with the metric of fairness that it uses. In order to mimic their analysis more closely, we will use ProPublica's definitions of \"high\" and \"low\" COMPAS scores:\n",
        "- Any score greater than 4 is considered a **high** score. A defendant with a high score is **predicted to recidivate.**\n",
        "- Any score less than or equal to 4 is considered a **low** score. A defendant with a low score is **predicted not to recidivate.**"
      ],
      "metadata": {
        "id": "Z50rH0WjhLa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Data Analysis\n",
        "\n",
        "Let's do some basic data analysis. First we'll plot histograms by \"sex\" and \"race\" (the terms used in the dataset). (Recall that a histogram indicates the number of observations with each value (or each range of values).) In previous notebooks, we used the `hist()` function from the `pandas` package, but this time we'll use the `histplot()` function in the `seaborn` package. They're similar, but the `seaborn` version is a little prettier and easier to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "lFiE8mWSb486"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ¤“ **Nerd detail:** The package is called `seaborn`, but when we imported it, we told Python to abbreviate it as `sns`, which is why it says `sns` below. It's common to abbreviate package names, such as `np` for `numpy`. The abbreviation `sns` for `seaborn` is traditional but non-intuitive; it turns out it's the initials of the character Sam Seaborn from *The West Wing*, after whom the developer of `seaborn` named the package. Coders are fond of inside jokes and references.\n",
        "---"
      ],
      "metadata": {
        "id": "ADoDaryYE4WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a histogram by sex. (stat=\"percent\" tells seaborn to \n",
        "# plot percentages instead of total counts in each bin.)\n",
        "sns.histplot(pp_data, x=\"sex\", discrete=True, stat=\"percent\");"
      ],
      "metadata": {
        "id": "PYWmbMAgcHKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ¤“ **Nerd detail:** You might be wondering why there's a semicolon (`;`) at the end of the line of code above, since Python (unlike some other programming languages) doesn't usually use semicolons. If we omit it, the Jupyter notebook will print an extra line of text output that doesn't mean much to us, and the semicolon suppresses it. (Try removing it and re-running the cell, if you want.) It's not a big deal either way.\n",
        "---"
      ],
      "metadata": {
        "id": "79_MsMSHIuoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 2a**\n",
        "\n",
        "Roughly what percentage of defendants in the dataset are male?\n"
      ],
      "metadata": {
        "id": "7qzcm3gOFq9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "aXLIrm0VFyTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2b**\n",
        "\n",
        "Plot a histogram by race. (Remember that we already filtered the data to include only rows in which \"race\" is Caucasian or African-American.) "
      ],
      "metadata": {
        "id": "g_91Ce6-GMbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "O9ZZ4-ZrGxrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2c**\n",
        "\n",
        "Roughly what percentage of defendants in the dataset are African-American?"
      ],
      "metadata": {
        "id": "Tm3xBYtSG18m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "HkoCcZs8Gwlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2d**\n",
        "\n",
        "Plot a histogram by \"`decile_score`\"â€”that's the COMPAS score."
      ],
      "metadata": {
        "id": "7Z1tYFT4HIK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "eoq_VR5yD1_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2e**\n",
        "\n",
        "Are the scores skewed toward lower risk or higher risk?"
      ],
      "metadata": {
        "id": "KcY8P0x7VSHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "7RRECRr9VSHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of Disparity\n",
        "\n",
        "Let's visualize how ProPublica began its investigation of racial disparity within the COMPAS risk assessment. The histogram below separates the one that you created above by race, displaying the differences between the risk scores of Black and white defendants. "
      ],
      "metadata": {
        "id": "v09rsSxxhSo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a histogram for the risk scores for Black defendants.\n",
        "sns.histplot(pp_data[pp_data[\"race\"] == \"African-American\"], x=\"decile_score\", discrete=True, color='orange', alpha=0.5)\n",
        "\n",
        "# Create a histogram for the risk scores for white defendants and\n",
        "# display it on the same plot as the plot for Black defendants.\n",
        "sns.histplot(pp_data[pp_data[\"race\"] == \"Caucasian\"], x=\"decile_score\", discrete=True, alpha=0.5)\n",
        "\n",
        "# Add legend.\n",
        "plt.legend(labels=[\"Black\", \"white\"]);"
      ],
      "metadata": {
        "id": "gYvKGBNFjsO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 3a**\n",
        "\n",
        "Is one racial group more likely to get a high risk score than the other? If so, does this by itself imply that the COMPAS model is biased? Why or why not?"
      ],
      "metadata": {
        "id": "uLVSdP7Q1lAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "0xMUbhat19e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3b**\n",
        "\n",
        "Assuming the risk scores are accurate, what might make one racial group more likely to get a high risk score than the other? (*Hint*: connect this to your knowledge of the history of policing and institutionalized racism.)"
      ],
      "metadata": {
        "id": "XANoJ1JT2CR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "Ehqq-skR2oiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The COMPAS Predictions\n",
        "\n",
        "At the moment, our dataset contains a column that tells us the COMPAS score (1â€“10), but it will be convenient for us to translate this into a prediction. (Remember that a defendant is predicted to recidivate if their COMPAS score is greater than 4.)"
      ],
      "metadata": {
        "id": "Z9bEZ9o_OvUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 4\n",
        "pp_data['COMPAS_prediction'] = 1 * (pp_data['decile_score'] > threshold)"
      ],
      "metadata": {
        "id": "MOhHUC9VOLsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a little tricky, but you'll need some of the tricks in your code below, so let's unpack it:\n",
        "\n",
        "* First, we create a new variable called `threshold` and set it to 4. We'll compare scores to this variable in the next line. Strictly speaking, we don't need to do thisâ€”we could just put `4` in the next line instead of `threshold`â€”but creating the variable makes the code more flexible and easier to understand.\n",
        "* The second line adds a new column to the dataset called `'COMPAS_prediction'`. (Or, if we have already added such a column, it replaces its data.) This column will contain a 1 if the defendant's `decile_score` is greater than `threshold` and to 0 otherwise. Here's how it works:\n",
        "* The code `pp_data['decile_score'] > threshold` essentially creates a new column. Each value in the column equals `True` if the `decile_score` is greater than `threshold` and `False` otherwise.\n",
        "* But the code we use below will need the predictions to be labeled as 1 and 0, not `True` and `False`. Multiplying `pp_data['decile_score'] > threshold` by 1 converts to 1 and 0.\n",
        "* The second line therefore sets `pp_data['COMPAS_prediction']` equal to this new column of 1s and 0s."
      ],
      "metadata": {
        "id": "deOOXB_1QYQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the dataset and confirm that the new column equals 1 when the `decile_score` is greater than 4 and equals 0 otherwise."
      ],
      "metadata": {
        "id": "qa_eYQ4HR_lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data"
      ],
      "metadata": {
        "id": "7kJFDQ56QMkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy, FPR, and FNR\n",
        "\n",
        "Recall that predictive models use algorithms to predict a target feature (in this case, whether or not the defendant will recidivate). We can evaluate how well a model predicts these features using **goodness metrics.** Goodness metrics typically summarize discrepancies between actual values and predicted values. Metrics like these are quite important in data science and are typically used by data scientists to determine how accurate and effective an algorithm is at predicting its predetermined goal.\n",
        "\n",
        "In our case, we want to examine the performance of `COMPAS_prediction` as a predictor of `two_year_recid`. \n",
        "We'll start with simply calculating the **accuracy** of the model, defined as the number of correct predictions divided by the total number of predictions.\n",
        "\n",
        "(More information about how to calculate the accuracy and other metrics we will discuss below can be found [here](https://en.wikipedia.org/wiki/Confusion_matrix).)"
      ],
      "metadata": {
        "id": "-GjDnpoa2pDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_predictions = pp_data[pp_data['two_year_recid'] == pp_data['COMPAS_prediction']]\n",
        "accuracy = len(correct_predictions) / len(pp_data)\n",
        "print(f\"accuracy = {accuracy}\")"
      ],
      "metadata": {
        "id": "_mExD5WQSorj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's unpack the code above:\n",
        "\n",
        "* In the first line, `pp_data['two_year_recid'] == pp_data['COMPAS_prediction']` is a like a new column that is set to `True` if `two_year_recid` and `COMPAS_prediction` are equal and is set to `False` otherwise. (`==` is Python for \"is equal to.\" `!=` means \"is not equal to.\")\n",
        "* Writing `pp_data[*a column that contains True and False*]` means \"only take the rows for which the column equals `True`.\"\n",
        "* So the first line creates a new dataset called `correct_predictions`, which is the original dataset filtered so that it only includes rows in which `two_year_recid` and `COMPAS_prediction` are equal.\n",
        "* The second line counts the number of rows in the `correct_predictions` dataset and divides it by the number in the original dataset to get the accuracy."
      ],
      "metadata": {
        "id": "rcDiEf2pVEA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The calculation reveals that the overall accuracy is 0.658, or 65.8%. "
      ],
      "metadata": {
        "id": "fjRm8SgJWSXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 4a**\n",
        "\n",
        "Do you think that 65.8% is a reasonable accuracy for a model such as COMPAS?"
      ],
      "metadata": {
        "id": "cNW8MDidW_fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "MXH9Z8cmXN_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4b**\n",
        "\n",
        "Write code to calculate the accuracy *only* for Black defendants. Set this equal to a variable called `accuracy_black`. \n",
        "\n",
        "Write code to calculate the accuracy *only* for white defendants. Set this equal to a variable called `accuracy_white`.\n",
        "\n",
        "*Hint 1*: Create new datasets called `pp_data_black` and `pp_data_white` that contain only the rows of `pp_data` whose `race` column equals \"African-American\" or \"Caucasian\", respectively. (That is, create new datasets that *filter* by race.) Then perform the accuracy calculation on each of those datasets instead of on `pp_data`. The code block below gets you started.\n",
        "\n",
        "*Hint 2*: Remember, to create a new dataset that filters by a certain condition, you can use code like:\n",
        "\n",
        "```\n",
        "pp_data[pp_data[\"age\"] >= 25]\n",
        "pp_data[pp_data[\"score_text\"] == \"Medium\"]\n",
        "pp_data[pp_data[\"two_year_recid\"] == pp_data[\"COMPAS_prediction\"]\n",
        "```\n",
        "\n",
        "*Hint 3*: You should find that the accuracy for Black defendants is 0.6491. If not, something is wrong."
      ],
      "metadata": {
        "id": "Z4JY3FiHXXyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data_black = [...]\n",
        "correct_predictions_black = [...]\n",
        "accuracy_black = [...]"
      ],
      "metadata": {
        "id": "B2543-MpYNOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data_white = [...]\n",
        "correct_predictions_white = [...]\n",
        "accuracy_white = [...]"
      ],
      "metadata": {
        "id": "yX3iTjFYYTUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, the model seems pretty fair, in the sense that the accuracy is similar for Black and white defendants. \n",
        "\n",
        "Let's dig a little deeper and look at **false positives (FP)** and **false negatives (FN)**. We'll need to know the numbers of false positives, true positives, false negatives, and true negativesâ€”overall, and for Black defendants, and for white defendants. We could write code for all of these...but fortunately there is a shortcut: We can use the `sklearn` function `confusion_matrix()`. \n",
        "\n",
        "Each defendant has a value for `COMPAS Decision` (either 0 or 1) and a value for `two_year_recid` (either 0 or 1). A *confusion matrix* shows how these values line up with each otherâ€”i.e., it shows  true and false positives and negatives. The **rows** represent the number of observations for which the **prediction** is 0 or 1, and the **columns** represent the number of observations for which the **actual** value is 0 or 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "D9wcOOQ-6Udn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(pp_data['two_year_recid'], pp_data['COMPAS_prediction'])\n",
        "cm"
      ],
      "metadata": {
        "id": "MVUxqAazgmbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results tell us that there are 1872 true negatives (prediction and actual = 0), 923 false negatives (prediction = 0, actual = 1), etc.\n",
        "\n",
        "We can store these values in separate variables using the `ravel()` function, which flattens the matrix into individual values:"
      ],
      "metadata": {
        "id": "911yXnkYhI-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TN, FP, FN, TP = cm.ravel()\n",
        "print(f\"There are:\\n{TN} true negatives\\n{FP} false positives\\n{FN} false negatives\\n{TP} true positives\")"
      ],
      "metadata": {
        "id": "zavWJ0OQgmQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dig a little deeper into these numbers. \n",
        "\n"
      ],
      "metadata": {
        "id": "B86a5fRniolF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = TN + FP + FN + TP\n",
        "print(f\"{(TN + FN)/total} predicted not to recidivate\")\n",
        "print(f\"{(TP + FP)/total} predicted to recidivate\")\n",
        "print(f\"{(TN + FP)/total} did not recidivate\")\n",
        "print(f\"{(TP + FN)/total} did recidivate\")"
      ],
      "metadata": {
        "id": "CwShDr4Tivhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, 52.2% of defendants were predicted not to recidivate (these are the true negatives and false negatives) and 47.8% were predicted to recidivate (true and false positives). This is pretty close to the actual precentages who did not recidivate (true negatives and false positives), 53.0%, and who did recidivate (true positives and false negatives), 47.0%."
      ],
      "metadata": {
        "id": "eEQ-QwRMioe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Overall accuracy = {(TN + TP)/total}\")"
      ],
      "metadata": {
        "id": "oJUD2-vCioUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, the overall accuracy is relatively low: 65.8%. (We also calculated this earlier.)"
      ],
      "metadata": {
        "id": "KDRyudDMkKgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ProPublica used the **false positive rate (FPR)** and **false negative rate (FNR)** as their metrics to understand and quantify fairness.\n",
        "\n",
        "In terms of COMPAS, the FPR is the proportion (i.e., the percentage, but expressed as a decimal) of non-recidivating defendants who were predicted to recidivate by the model. Mathematically,\n",
        "$$\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}},$$\n",
        "where $\\text{FP}$ is the number of false positives (people who were predicted to recidivate but did not) and $\\text{TN}$ is the number of true negatives (people who were predicted not to recidivate and did not).\n",
        "\n",
        "In contrast, the FNR is the proportion of recidivating defendants who were predicted not to recidivate by the model:\n",
        "$$\\text{FNR} = \\frac{\\text{FN}}{\\text{FN}+\\text{TP}},$$\n",
        "where $\\text{FN}$ is the number of false negatives (people who were predicted not to recidivate but did) and $\\text{TP}$ is the number of true positives (people who were predicted to recidivate and did)."
      ],
      "metadata": {
        "id": "dHA1AatzgDaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FPR = FP / (FP + TN)\n",
        "FNR = FN / (TP + FN)\n",
        "print(f\"Among all non-recidivating defendants, {FPR} were predicted (incorrectly) to recidivate\")\n",
        "print(f\"Among all recidivating defendants, {FNR} were predicted (incorrectly) not to recidivate\")"
      ],
      "metadata": {
        "id": "kg-_qYICnMnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, among all defendants who did not recidivate, 33.0% were falsely predicted to recidivate (the FPR); and among all defendants who did recidivate, 35.5% were false predicted not to recidivate (the FNR). The fact that these are roughly equal is another relatively good sign."
      ],
      "metadata": {
        "id": "8RoTTSNjntnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does COMPAS Overpredict or Underpredict across Groups?"
      ],
      "metadata": {
        "id": "yJq7rSJ-pTnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 5a**\n",
        "\n",
        "Calculate the FPR and FNR for Black defendants only.  \n",
        "\n",
        "*Hint*: When we calcluated FPR and FNR for *all* defendants, we used 3 steps:\n",
        "1. Calculate the confusion matrix `cm`.\n",
        "2. Extract `TN`, `FP`, `FN`, and `TP` using `ravel()`.\n",
        "3. Calculate `FPR` and `FNR` from these.\n",
        "\n",
        "Do the same 3 steps, but start with the dataset `pp_data_black` that you created earlier, instead of using `pp_data`."
      ],
      "metadata": {
        "id": "EqKY1cNhppPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "mVd2rRwgk7hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5b**\n",
        "\n",
        "Do the same thing, but for white defendants only."
      ],
      "metadata": {
        "id": "PSfML7DmfXN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "QcDbvb4nfYVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your results should align with the table in the ProPublica article titled \"Prediction Fails Differently for Black Defendants\". Your results will differ a bit, due to small differences in the way the dataset was filtered."
      ],
      "metadata": {
        "id": "TwuAfbf4qsXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5c**\n",
        "\n",
        "What do you notice about these percentages? Are they different across racial groups?"
      ],
      "metadata": {
        "id": "DxwoV8fL-Qny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "Icb-PkIDFPPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5d**\n",
        "\n",
        "What can you conclude from these metrics about the *overprediction* of risk scores for Black and white defendants? "
      ],
      "metadata": {
        "id": "1rn_uulmFPhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "ybLmERmGRlI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5e**\n",
        "\n",
        "What can you conclude from these metrics about the *underprediction* of risk scores for Black and white defendants?"
      ],
      "metadata": {
        "id": "6MMYzfwpRokg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "YbU0Om-SRuK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ROC Curve\n",
        "\n",
        "Let's plot the ROC curve for the COMPAS model. Recall that the ROC curve plots the TPR (the true positive rate, defined as $\\text{TP}/(\\text{TP}+\\text{FN})$) vs. the FPR at different values of the threshold. A predictive model that is very good at predicting will have its ROC curve pushed up and to the left, whereas a poor model will be closer to the diagonal line that goes from the bottom-left to top-right.\n",
        "\n",
        "\n",
        "`sklearn` has built-in functionsâ€”`roc_curve()` and `roc_auc_score()` to do the calculations for us."
      ],
      "metadata": {
        "id": "dUmzbMh17cNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC curve data.\n",
        "FPR, TPR, _ = roc_curve(pp_data['two_year_recid'], pp_data['decile_score'])\n",
        "# Calculate AUC.\n",
        "AUC = roc_auc_score(pp_data['two_year_recid'], pp_data['decile_score'])\n",
        "# Plot curve.\n",
        "plt.plot(FPR, TPR)\n",
        "plt.plot([(0, 0), (1, 1)], 'r--')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title(f'ROC Curve: COMPAS\\n AUC: {AUC:.4f}');"
      ],
      "metadata": {
        "id": "r2zYLrtS3O-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An AUC of 0.71 is borderline in terms of whether it would be considered acceptable. The low AUC is a result of the overlap between the two groups in the histogram: If there were a cleaner separation in the histogram between defendants who recidivated and who did not, the ROC would be pushed upward and to the left, and the AUC would be larger."
      ],
      "metadata": {
        "id": "ZK2OqH4_-Ubd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing a Threshold"
      ],
      "metadata": {
        "id": "heWsfDsMyPBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ðŸ›‘ **STOP!** This section is **entirely optional.** If you've been enjoying the coding so far and want to keep going, go for it. (The coding that comes next isn't particularly easier or harder than what you've done already.) If not, skip to the section on \"How to Submit this Notebook\".\n",
        "---"
      ],
      "metadata": {
        "id": "h-4xcBlZgagw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Question 3d, you built a histogram of the COMPAS scores for all defendants. Then, we built a histogram that separates the data by race.\n",
        "\n",
        "Now let's separate the data in a different way, based on whether or not the defendant recidivated. The plot below shows the frequency of decile scores (COMPAS scores) for defendants who did not recidivate (purple bars) and for those who did (green bars)."
      ],
      "metadata": {
        "id": "VKm45EmKyv44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(pp_data[pp_data[\"two_year_recid\"] == 0], x=\"decile_score\", discrete=True, color='purple', alpha=0.5)\n",
        "sns.histplot(pp_data[pp_data[\"two_year_recid\"] == 1], x=\"decile_score\", discrete=True, color='green', alpha=0.5)\n",
        "plt.legend(labels=[\"did not recidivate\", \"did recidivate\"])\n",
        "plt.axvline(threshold+0.5, color='k'); # (threshold was assigned above)"
      ],
      "metadata": {
        "id": "kDgSHxTHoyON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the aggregate, the model assigned lower decile scores to defendants who would up not recidivating and higher decile scores for defendants who did. That's good. But there is a lot of overlap between the two groups of bars, suggesting that the model is not a particularly accurate predictor.\n",
        "\n",
        "COMPAS basically draws a line right after a specific decile score and says \"anybody above this line is predicted to recidivate; anybody below it is predicted not to.\" Northpointe set this line right after 4. This is called a **threshold model.**\n",
        "\n",
        "Let's calculate our own threshold model. First we'll set the threshold to 4 to confirm that we get the same results as COMPAS."
      ],
      "metadata": {
        "id": "54blI2Df0Ztr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "**Question 6a**\n",
        "\n",
        "Create a new variable called `my_threshold` and set it equal to 4.\n"
      ],
      "metadata": {
        "id": "TEOESaSi18GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "5vRPnGei1kPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6b** \n",
        "\n",
        "Write code to add a new column called `my_prediction` to `pp_data`; the values in the column should be set to 1 if the corresponding value of `decile_score` is greater than `my_threshold` and should be set to 0 otherwise.\n",
        "\n",
        "*Hint*: Look back at the section titled \"The COMPAS Predictions.\"\n",
        "\n",
        "(You might want to view some rows of the new dataset to make sure your new column is set correctly.)"
      ],
      "metadata": {
        "id": "vJadvF_D2NZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "Qzrknt_coyKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a quick check to make sure that `my_prediction` currently agrees with `COMPAS_prediction`. (It should, since we're using the same threshold at the moment.)"
      ],
      "metadata": {
        "id": "7C6022hB28Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Does `COMPAS_prediction` column equal `my_prediction` column?\n",
        "pp_data['COMPAS_prediction'].equals(pp_data['my_prediction'])"
      ],
      "metadata": {
        "id": "I74RCzC4oyHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's another histogram. (It's the same as the histogram we plotted above, but the line is drawn at your threshold instead of Northpointe's.)"
      ],
      "metadata": {
        "id": "QJJ8l33K4jK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(pp_data[pp_data[\"two_year_recid\"] == 0], x=\"decile_score\", discrete=True, color='purple', alpha=0.5)\n",
        "sns.histplot(pp_data[pp_data[\"two_year_recid\"] == 1], x=\"decile_score\", discrete=True, color='green', alpha=0.5)\n",
        "plt.legend(labels=[\"did not recidivate\", \"did recidivate\"])\n",
        "plt.axvline(my_threshold+0.5, color='k'); "
      ],
      "metadata": {
        "id": "DPAiH6BI4igF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6c**\n",
        "\n",
        "For the model that uses `my_threshold`, calculate the FPR and FNR separately for Black and white defendents.\n",
        "\n",
        "*Hint*: When you added the new column to `pp_data`, it did *not* automatically add this column to your datasets `pp_data_black` and `pp_data_white`. You'll have do add it yourself. You can either add it the same way you added it to `pp_data`, or you can create new versions of `pp_data_black` and `pp_data_white` based on the new `pp_data`, like you did earlier."
      ],
      "metadata": {
        "id": "hO0YXKpo4JTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[...]"
      ],
      "metadata": {
        "id": "uGSlOcus6IBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6d**\n",
        "\n",
        "Now try other thresholds. Experiment with the value of `my_threshold`. For each value of `my_threshold` that you try, calculate the FPR and FNR. \n",
        "\n",
        "What value of `my_threshold` would you recommend, if you were designing the COMPAS system? For the value you choose, report the FPR and FNR."
      ],
      "metadata": {
        "id": "G2OoSiMS3bRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** *YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "igS-w_NN7ZQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## How to Submit this Notebook\n",
        "\n",
        "* Please click the \"Share\" button at the top of the page. In the Share options:\n",
        "\n",
        "    * Under \"General Access\", change to \"Restricted\" (instead of \"Anyone with the link\").\n",
        "    * At the top, share it with Oumaima (ous219@lehigh.edu), Larry (lvs2@lehigh.edu), and Suzanne (sme6@lehigh.edu).\n",
        "    * Click \"Copy Link\".\n",
        "    * Click \"Done\".\n",
        "\n",
        "* Send a Slack DM to Oumaima, Larry, Suzanne, and your partner with the link you copied.\n"
      ],
      "metadata": {
        "id": "Zu0D5oz5OsYx"
      }
    }
  ]
}